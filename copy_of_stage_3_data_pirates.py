# -*- coding: utf-8 -*-
"""Copy of STAGE 3- Data pirates.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EsZGTzbc3l2nWTvpZ0uTU_SBTeOqZz12

#opening
"""

!gdown --id 1Hrzb8-td5D0P4GLks3QBtzkCUzpszzP3

!pip install shap

#Data manipulation

import pandas as pd
import numpy as np

#Data visualization

import matplotlib.pyplot as plt
import seaborn as sns

#Warning

import warnings 
warnings.simplefilter(action = 'ignore', category = FutureWarning)

#Preparation
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report

!pip install xgboost

!pip install sklearn

df = pd.read_csv('/content/TravelInsurancePrediction.csv')
df.head()

df = df.drop_duplicates(subset = ['Age','Employment Type','GraduateOrNot','AnnualIncome','FamilyMembers','ChronicDiseases','FrequentFlyer','EverTravelledAbroad',"TravelInsurance"])
df.shape

df['Employment Type']=df['Employment Type'].map({'Private Sector/Self Employed':1,'Government Sector':0})
df['GraduateOrNot']=df['GraduateOrNot'].map({'Yes':1,'No':0})
df['FrequentFlyer']=df['FrequentFlyer'].map({'No':0,'Yes':1})
df['EverTravelledAbroad']=df['EverTravelledAbroad'].map({'No':0,'Yes':1})
df.head()

ft = [ 'Age', 'Employment Type', 'GraduateOrNot', 'AnnualIncome','FamilyMembers', 'ChronicDiseases', 'FrequentFlyer', 'EverTravelledAbroad']
x = df[ft]
y = df.TravelInsurance

train_x,test_x,train_y,test_y = train_test_split(x, y, test_size=1/3, random_state=42) #split data

print("train_x size : ",train_x.size)
print("test_x size  : ",test_x.size)
print("train_y size : ",train_y.size)
print("test_y size  : ",test_y.size)

pd.DataFrame(train_y)

print(pd.Series(train_y).value_counts())

"""# Oversampling Data Split dengan SMOTE"""

from imblearn.over_sampling import SMOTE
X_train_over, y_train_over = SMOTE().fit_resample(train_x, train_y)

# Sebelum Oversampling
print('Target sebelum oversampling:')
print(pd.Series(train_y).value_counts())

# Setelah Oversampling
print('Target setelah oversampling:')
print(pd.Series(y_train_over).value_counts())

y_train_over

X_train_over.info()

"""#DATA MODELLING"""

#model evaluation classification test (default)
def eval_classification(model):
    y_pred = model.predict(test_x)
    y_pred_train = model.predict(train_x)
    y_pred_proba = model.predict_proba(test_x)
    y_pred_proba_train = model.predict_proba(train_x)
    
    print("Accuracy (Test Set): %.2f" % accuracy_score(test_y, y_pred))
    print("Accuracy (Train Set): %.2f" % accuracy_score(train_y, y_pred_train))
    
    print("Precision (Test Set): %.2f" % precision_score(test_y, y_pred))
    print("Precision (Train Set): %.2f" % precision_score(train_y, y_pred_train))
    
    print("Recall (Test Set): %.2f" % recall_score(test_y, y_pred))
    print("Recall (Train Set): %.2f" % recall_score(train_y, y_pred_train))
    
    print("F1-Score (Test Set): %.2f" % f1_score(test_y, y_pred))
    print("F1-Score (Train Set): %.2f" % f1_score(train_y, y_pred_train))
    
    print("roc_auc (test-proba): %.2f" % roc_auc_score(test_y, y_pred_proba[:, 1]))
    print("roc_auc (train-proba): %.2f" % roc_auc_score(train_y, y_pred_proba_train[:, 1]))

    score = cross_validate(model, x, y, cv=5, scoring='roc_auc', return_train_score=True)
    print('roc_auc (crossval train): '+ str(score['train_score'].mean()))
    print('roc_auc (crossval test): '+ str(score['test_score'].mean()))

    # Membuat Confusion Matrix (Test)
    cnf_matrix = confusion_matrix(test_y, y_pred)
    print(classification_report(test_y, y_pred))
    sns.heatmap(cnf_matrix,cmap='coolwarm_r',annot=True,linewidth=0.5,fmt='d')
    plt.title('Confusion Matrix (Test)')
    plt.xlabel('Prediksi')
    plt.ylabel('Aktual')

    # Membuat Confusion Matrix (Train)
    #cnf_matrix = confusion_matrix(train_y, y_pred_train)
    #print(classification_report(train_y, y_pred_train))
    #sns.heatmap(cnf_matrix,cmap='coolwarm_r',annot=True,linewidth=0.5,fmt='d')
    #plt.title('Confusion Matrix (Train)')
    #plt.xlabel('Prediksi')
    #plt.ylabel('Aktual')
    
def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=x.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model):
    print(model.best_estimator_.get_params())

"""#logistic Regression"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(train_x,train_y)

eval_classification(logreg)

"""# ML Supervised Model

## Logistic Regression
"""

from sklearn.linear_model import LogisticRegression 
logreg = LogisticRegression(random_state = 42)#, class_weight='balanced')
logreg.fit(train_x, train_y)
eval_classification(logreg)

"""### Hyperparameter tuning"""

from sklearn.model_selection import RandomizedSearchCV

penalty = ['l1', 'l2']
C = [float(x) for x in np.linspace(0.001, 0.05, 100)]
hyperparameters = dict(penalty=penalty, C=C)

logreg = LogisticRegression()
rs = RandomizedSearchCV(logreg, hyperparameters, scoring='roc_auc', random_state=1, cv=5, n_iter=50)
rs.fit(train_x, train_y)
eval_classification(rs)

"""##Decision Tree"""

dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_x, train_y)
eval_classification(dt)

"""### Hyperparameter tuning"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy.stats import uniform
import numpy as np

#list hyperparameter
max_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree
min_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node
min_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node
max_features = ['auto', 'sqrt'] # Number of features to consider at every split
criterion = ['gini','entropy']
splitter = ['best','random']

hyperparameters = dict(max_depth=max_depth, 
                       min_samples_split=min_samples_split, 
                       min_samples_leaf=min_samples_leaf,
                       max_features=max_features,
                       criterion=criterion,
                       splitter=splitter
                      )

# Inisialisasi Model
dt = DecisionTreeClassifier(random_state=42)
model = RandomizedSearchCV(dt, hyperparameters, cv=5, scoring='precision')
model.fit(train_x, train_y)

# Predict & Evaluation
y_pred = model.predict(train_x)#Check performa dari model
eval_classification(model)

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=x.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

show_feature_importance(model.best_estimator_)

"""##AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(random_state=42)
ada.fit(train_x, train_y)
eval_classification(ada)

"""##XGboost"""

from xgboost import XGBClassifier

xg = XGBClassifier(random_state=42)
xg.fit(train_x, train_y)
eval_classification(xg)

"""### Hyperparameter tuning"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import numpy as np

hyperparameters = {
                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],
                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],
                    'gamma' : [float(x) for x in np.linspace(0, 11, num = 11)],
                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],

                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],

                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)]
                    }

# Init
from xgboost import XGBClassifier
xg = XGBClassifier(random_state=42)
xg_tuned = RandomizedSearchCV(xg, hyperparameters, cv=10, random_state=42, scoring='recall')
xg_tuned.fit(train_x,train_y)

# Predict & Evaluation
eval_classification(xg_tuned)

"""##Random Forest Model"""

rf = RandomForestClassifier(random_state=42)
rf.fit(train_x, train_y)
eval_classification(rf)

"""# Model Using SMOTE

## Logistic Regression Using SMOTE
"""

lrs = LogisticRegression(random_state = 42)#, class_weight='balanced')
lrs.fit(X_train_over, y_train_over)
eval_classification(lrs)

"""## Decision Tree Using SMOTE"""

dts = DecisionTreeClassifier(random_state=42)
dts.fit(X_train_over, y_train_over)
eval_classification(dts)

"""## AdaBoost Using SMOTE"""

adas = AdaBoostClassifier(random_state=42)
adas.fit(X_train_over, y_train_over)
eval_classification(adas)

"""## XGBoost Using SMOTE"""

xgs = XGBClassifier(random_state=42)
xgs.fit(X_train_over, y_train_over)
eval_classification(xgs)

"""### hyper tuning xgboost"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import numpy as np

hyperparameters = {
                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],
                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],
                    'gamma' : [float(x) for x in np.linspace(0, 11, num = 11)],
                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],

                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],

                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)]
                    }

# Init
from xgboost import XGBClassifier
xg = XGBClassifier(random_state=42)
xg_tuned = RandomizedSearchCV(xg, hyperparameters, cv=10, random_state=42, scoring='recall')
xg_tuned.fit(X_train_over, y_train_over)

# Predict & Evaluation
eval_classification(xg_tuned)

"""### learning curve xgboost gamma"""

param_values = [int(x) for x in np.linspace(0, 50, 100)] 

train_scores = []
test_scores = []

for i in param_values:
    model = XGBClassifier(n_estimators=75, gamma=i)
    model.fit(X_train_over, y_train_over)

    # train evaluation
    y_pred_train_proba = model.predict_proba(X_train_over)
    train_auc = roc_auc_score(y_train_over, y_pred_train_proba[:,1])
    train_scores.append(train_auc)

    # test evaluation
    y_pred_proba = model.predict_proba(test_x)
    test_auc = roc_auc_score(test_y, y_pred_proba[:,1])
    test_scores.append(test_auc)

    print('param value: ' + str(i) + '; train: ' + str(train_auc) + '; test: '+ str(test_auc))

plt.plot(param_values, train_scores, label='Train')
plt.plot(param_values, test_scores, label='Test')
plt.ylabel('AUC')
plt.xlabel('XGBoost')
plt.legend()
plt.show()

"""##Random Forest Using SMOTE"""

rf = RandomForestClassifier(random_state=42)
rf.fit(train_x, train_y)
eval_classification(rf)

"""# Hyperparameter tuning"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

n_estimators = [int(x) for x in np.linspace(700, 800, 50)]
criterion = ['gini', 'entropy']
max_depth = [int(x) for x in np.linspace(2, 100, 50)]
min_samples_split = [int(x) for x in np.linspace(2, 20, 10)]
min_samples_leaf = [int(x) for x in np.linspace(2, 20, 10)]
hyperparameters = dict(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,
                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)

rf = RandomForestClassifier(random_state=42)
rs = GridSearchCV(rf, hyperparameters, scoring='roc_auc', cv=5)
rs.fit(X_train_over, y_train_over)
eval_classification(rs)

"""#Learning Curve"""

param_values = [int(x) for x in np.linspace(10, 40, 50)] 

train_scores = []
test_scores = []

for i in param_values:
    model = RandomForestClassifier(n_estimators=75, min_samples_leaf=i)
    model.fit(X_train_over, y_train_over)

    # train evaluation
    y_pred_train_proba = model.predict_proba(X_train_over)
    train_auc = roc_auc_score(y_train_over, y_pred_train_proba[:,1])
    train_scores.append(train_auc)

    # test evaluation
    y_pred_proba = model.predict_proba(test_x)
    test_auc = roc_auc_score(test_y, y_pred_proba[:,1])
    test_scores.append(test_auc)

    print('param value: ' + str(i) + '; train: ' + str(train_auc) + '; test: '+ str(test_auc))

plt.plot(param_values, train_scores, label='Train')
plt.plot(param_values, test_scores, label='Test')
plt.ylabel('AUC')
plt.xlabel('RandomForest')
plt.legend()
plt.show()

"""#Feature Importance"""

show_feature_importance(rf)

"""5 Feature importance tertinggi adalah AnnualIncome, Family Members, Age, Ever Travelled Abroad, Chronic Diseases."""

import shap
explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(test_x)
shap.summary_plot(shap_values[1], test_x)

"""# Business Insight

Dari hasil analisa, perusahaan bisa fokus pada pelanggan yang memiliki karakteristik gabungan dari 5 feature importance tertinggi (Age, annual income, ever travelled abroad, Family members, Chronic diseases) agar bisa menarik pelanggan baru yang tepat sasaran. 

Dalam hal prospecting, perusahaan bisa memfokuskan penjualan ke calon klien yang sudah berkeluarga serta cenderung memiliki penyakit kronis yang sesuai dengan paket perlindungan asuransi dan pernah berpergian misalnya dengan memberikan promo terkait tiket perjalanan yang lebih murah apabila calon klien membeli paket asuransi travel. Hal tersebut dapat menarik minat pelanggan.
"""

